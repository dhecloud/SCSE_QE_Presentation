\thispagestyle{plain}
\begin{center}
    \vspace{0.9cm}
        \textbf{\Large{Abstract}}
\end{center}

Autoregressive methods have been the defacto decoding method for sequence generation. However, with the increasingly bigger models in the recent years, the need for non-autoregressive models has become even more pressing in order to save time and computational power. In this work, we examine transfer learning as a method to convert an autoregressive model to a non-autoregressive model. We also propose a novel decoding method for our architecture. Finally, we study the effect of freezing certain layers in the autoregressive models during transfer learning.